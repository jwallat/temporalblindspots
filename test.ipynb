{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    Answer the question.\n",
    "\n",
    "    ### Input:\n",
    "    {question}\n",
    "\n",
    "    ### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n    ### Instruction:\\n    Answer the question.\\n\\n    ### Input:\\n    {question}\\n\\n    ### Response:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Answer the question.\n",
      "\n",
      "### Input:\n",
      " {question}\n",
      "\n",
      "### Response:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Answer the question.\n",
      "\n",
      "### Input:\n",
      " Who is cool?\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "t = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the question.\\n\\n### Input:\\n {question}\\n\\n### Response:\"\n",
    "\n",
    "print(t)\n",
    "\n",
    "q = \"Who is cool?\"\n",
    "t2 = t.replace(\"{question}\", q)\n",
    "print(t2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/wallat/temporal-llms/prompts.json') as file:\n",
    "    json_data = json.load(file)\n",
    "# json_data = json.load('/home/wallat/temporal-llms/prompts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpaca-7b': {'default': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the question.\\n\\n### Input:\\n{question}\\n\\n### Response:',\n",
       "  'default_shorter_answer_instruction': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the question concisely.\\n\\n### Input:\\n{question}\\n\\n### Response:',\n",
       "  'no_input': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Response:',\n",
       "  'no_input_shorter_answer': 'Below is an instruction that describes a task. Write a short response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Response:'},\n",
       " 'text-davinci-003': {}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=f\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. \"\n",
    "f\"If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life\"\n",
    "f\" expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United\"\n",
    "f\" States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: He \"\n",
    "f\"Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: \"\n",
    "f\"Barcelona, Spain.\\n\\n\"\n",
    "\"Q: {question}\\nA: \"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: He Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: Barcelona, Spain.\\n\\nQ: {question}\\nA: \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_style = \"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: He Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: Barcelona, Spain.\\n\\nQ: {question}\\nA: \"\n",
    "\n",
    "gpt_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the question.\\n\\n### Input:\\nI am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: He Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: Barcelona, Spain.\\n\\nQ: {question}\\n\\n### Response:\",)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the question.\\n\\n### Input:\\nI am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: He Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: Barcelona, Spain.\\n\\nQ: {question}\\n\\n### Response:\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer the questions.\\n\\n### Input:\\nQ: What is human life expectancy in the United States?\\nA: 78 years\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower\\n\\nQ: Which party did he belong to?\\nA: Republican Party\\n\\nQ: Where were the 1992 Olympics held?\\nA: Barcelona, Spain.\\n\\nQ: {question}\\nA:\\n\\n### Response:\",\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with 'Unknown'.\\n\\nQ: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: Unknown\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: How many squigs are in a bonk?\\nA: Unknown\\n\\nQ: {question}?\\nA: \","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What submarine was the W-88 deployed on?', 'When was Felipe Guaman Poma de Ayala born?', 'Where was the Catholic University Law School located?', 'Who did Edward I. Koch call on to give Dinkins the honor of speaking at the event instead?', \"Where did Mrs. Liebenow and her daughter move to after her father's death?\", 'When did Little, Brown say it would reissue Pascual Duarte in hardcover and paperback?', \"What magazine bought General Powell's memoir?\", 'What country is Gerhard Simon a West German authority on?', 'Who would not request that American investigators be allowed to question the scientist at the center of the proliferation scandal?', 'Who worked with Herricks Middle School to start the TACK program?']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata\u001b[39m():\n\u001b[1;32m     12\u001b[0m     \u001b[39m# for question in data['question'][:10].iterrows():\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# #     # This could come from a dataset, a database, a queue or HTTP request\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#     print(question)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#     yield question\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39myield\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m10\u001b[39m]\u001b[39m.\u001b[39miterrows()\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m pipe(data()):\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/.conda/envs/temp/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/temp/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:34\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[1;32m     35\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/temp/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mdata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata\u001b[39m():\n\u001b[1;32m     12\u001b[0m     \u001b[39m# for question in data['question'][:10].iterrows():\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# #     # This could come from a dataset, a database, a queue or HTTP request\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#     print(question)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#     yield question\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[39myield\u001b[39;00m data[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m][:\u001b[39m10\u001b[39m]\u001b[39m.\u001b[39miterrows()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "pipe = pipeline(\"text-classification\")\n",
    "\n",
    "data = pd.read_csv('/home/wallat/temporal-llms/data/ArchivalQA/splits/ArchivalQANoTime/ArchivalQANoTime_test.csv')\n",
    "l = data['question'][:10].to_list()\n",
    "\n",
    "print(l)\n",
    "\n",
    "def data():\n",
    "    for question in l:\n",
    "    # #     # This could come from a dataset, a database, a queue or HTTP request\n",
    "    # #     # in a server\n",
    "    # #     # Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n",
    "    # #     # to use multiple threads to preprocess data. You can still have 1 thread that\n",
    "    # #     # does the preprocessing while the main runs the big inference\n",
    "    # #     yield question\n",
    "    #     print(question)\n",
    "    #     yield question\n",
    "        yield question\n",
    "\n",
    "\n",
    "for out in pipe(data()):\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
